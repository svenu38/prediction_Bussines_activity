{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirements loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i ../script/requirements.py\n",
    "%run -i ../script/requirements_dll.py\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../df_completed_order.csv', dtype = {'_CASE_KEY' : str, 'WERKS' : str, 'EBELN' : str, 'EBELP': str, 'ACTIVITY_EN': str, '_SORTING' : int, 'USER_NAME': str})\n",
    "df['EVENTTIME'] = pd.to_datetime(df['EVENTTIME'])\n",
    "df['BLDAT'] = pd.to_datetime(df['BLDAT'])\n",
    "df['BUDAT'] = pd.to_datetime(df['BUDAT'])\n",
    "df['BPDAT'] = pd.to_datetime(df['BPDAT'])\n",
    "df['DRDAT'] = pd.to_datetime(df['DRDAT'])\n",
    "df['LEWED'] = pd.to_datetime(df['LEWED'])\n",
    "df['SEDAT'] = pd.to_datetime(df['SEDAT'])\n",
    "df['LCWED'] = pd.to_datetime(df['LCWED'])\n",
    "df['ITDAT'] = pd.to_datetime(df['ITDAT'])\n",
    "df = df.rename(columns={'_CASE_KEY' : 'case:concept:name',\n",
    "                       'ACTIVITY_EN' : 'concept:name',\n",
    "                       'EVENTTIME' : 'time:timestamp'})\n",
    "df.loc[df['RKDST'].isnull(), 'RKDST'] = 0\n",
    "df.loc[df['RKDST'] == 'x', 'RKDST'] = 1\n",
    "df.loc[df['MAVPR'].isnull(), 'MAVPR'] = 0\n",
    "df.loc[df['THWMS'].isnull(), 'THWMS'] = 0\n",
    "df.loc[df['LATEP'].isnull(), 'LATEP'] = 0\n",
    "df['MAVPR'] = df['MAVPR'].astype(int)\n",
    "df['THWMS'] = df['THWMS'].astype(int)\n",
    "df['LATEP'] = df['LATEP'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[~df['concept:name'].isin(['Record Invoice','Cleared Invoice', 'Due Date passed', 'Remove Payment Block', 'Set Payment Block']), ['BLDAT','BUDAT','BPDAT']] = pd.NaT\n",
    "df.loc[df['BLDAT'].isna(), ['BLDAT', 'BUDAT', 'BPDAT']] = df['BLDAT'].iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time\n",
    "A = [d.date() for d in df['BLDAT']]\n",
    "B = [d.date() for d in df['BUDAT']]\n",
    "C = [d.date() for d in df['BPDAT']]\n",
    "df['time:diff_BUDAT'] = np.busday_count(A, B)\n",
    "df['time:diff_BPDAT'] = np.busday_count(A, C)\n",
    "sc_day = StandardScaler()\n",
    "df['time:diff_BPDAT'] = sc_day.fit_transform(np.array(df['time:diff_BPDAT']).reshape(-1, 1))\n",
    "df['time:diff_BUDAT'] = sc_day.transform(np.array(df['time:diff_BUDAT']).reshape(-1, 1))\n",
    "\n",
    "# Time\n",
    "A = [d.date() for d in df['DRDAT']]\n",
    "B = [d.date() for d in df['LEWED']]\n",
    "C = [d.date() for d in df['SEDAT']]\n",
    "df['time:diff_LEWED'] = np.busday_count(A, B)\n",
    "df['time:diff_SEDAT'] = np.busday_count(A, C)\n",
    "sc_day_ship = StandardScaler()\n",
    "df['time:diff_LEWED'] = sc_day_ship.fit_transform(np.array(df['time:diff_LEWED']).reshape(-1, 1))\n",
    "df['time:diff_SEDAT'] = sc_day_ship.transform(np.array(df['time:diff_SEDAT']).reshape(-1, 1))\n",
    "\n",
    "# Time\n",
    "A = [d.date() for d in df['DRDAT']]\n",
    "B = [d.date() for d in df['LCWED']]\n",
    "C = [d.date() for d in df['ITDAT']]\n",
    "df['time:diff_LCWED'] = np.busday_count(A, B)\n",
    "df['time:diff_ITDAT'] = np.busday_count(A, C)\n",
    "sc_day_del = StandardScaler()\n",
    "df['time:diff_LCWED'] = sc_day_del.fit_transform(np.array(df['time:diff_LCWED']).reshape(-1, 1))\n",
    "df['time:diff_ITDAT'] = sc_day_del.transform(np.array(df['time:diff_ITDAT']).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Scaler mean: \", sc_day.mean_)\n",
    "print(\"Scaler scale: \", sc_day.scale_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_sorting = StandardScaler()\n",
    "df['_SORTING'] = sc_sorting.fit_transform(np.array(df['_SORTING']).reshape(-1, 1))\n",
    "df = df.sort_values(by = ['WERKS', 'EBELN', 'EBELP'])\n",
    "\n",
    "df = df.loc[df['concept:name'] != 'Late Shipment'].sort_values(by = ['WERKS','EBELN', 'EBELP', 'time:timestamp', '_SORTING'])\n",
    "df = df.loc[df['concept:name'] != 'Late Delivery'].sort_values(by = ['WERKS','EBELN', 'EBELP', 'time:timestamp', '_SORTING'])\n",
    "df = df.loc[df['concept:name'] != 'Due Date passed'].sort_values(by = ['WERKS','EBELN', 'EBELP', 'time:timestamp', '_SORTING'])\n",
    "df = df.reset_index(drop= True)\n",
    "\n",
    "n_act, onehot_encoder_act = one_hot_column(df['concept:name'])\n",
    "n_wer, onehot_encoder_wer = one_hot_column(df['WERKS'])\n",
    "n_nam, onehot_encoder_nam = one_hot_column(df['NAME1'])\n",
    "n_mag, onehot_encoder_mag = one_hot_column(df['MATKL'])\n",
    "n_mat, onehot_encoder_mat = one_hot_column(df['MATNR'])\n",
    "\n",
    "# Time\n",
    "A = [d.date() for d in df['time:timestamp']]\n",
    "B = [d.date() for d in df['time:timestamp'].shift(-1)]\n",
    "B[-1] = B[-2]\n",
    "df['time:diff'] = np.busday_count(A, B)\n",
    "df['time:diff'] = df['time:diff'].shift(1)\n",
    "df.loc[0, 'time:diff'] = 0\n",
    "idx = df.reset_index().groupby('case:concept:name')['index'].first().values\n",
    "df.loc[df.index.isin(idx), 'time:diff'] = 0\n",
    "sc = StandardScaler()\n",
    "df['time:diff'] = sc.fit_transform(np.array(df['time:diff']).reshape(-1, 1))\n",
    "df['concept:name'] = df.groupby('case:concept:name')['concept:name'].transform(lambda x: ','.join(x))\n",
    "df['time:diff'] = df['time:diff'].astype(str)\n",
    "df['time:diff'] = df.groupby('case:concept:name')['time:diff'].transform(lambda x: ','.join(x))\n",
    "df['_SORTING'] = df['_SORTING'].astype(str)\n",
    "df['_SORTING'] = df.groupby('case:concept:name')['_SORTING'].transform(lambda x: ','.join(x))\n",
    "\n",
    "df = df[['case:concept:name', 'concept:name', 'RKDST', 'WERKS', 'time:diff', 'NAME1', 'MATKL', 'MATNR', 'MAVPR', 'THWMS', 'LATES','LATED','LATEP', '_SORTING', \n",
    "         'time:diff_BUDAT' , 'time:diff_BPDAT', 'time:diff_LEWED', 'time:diff_SEDAT', 'time:diff_LCWED', 'time:diff_ITDAT']].drop_duplicates().reset_index(drop = True)\n",
    "\n",
    "traces= []\n",
    "diff_time = []\n",
    "sorting = []\n",
    "for q in range(len(df)):\n",
    "    string = np.array(df.loc[q, 'concept:name'].split(','),dtype = str).reshape(-1,1)\n",
    "    onehot_encoded = onehot_encoder_act.transform(string)\n",
    "    traces.append(onehot_encoded)\n",
    "    diff_time.append(np.array(df.loc[q, 'time:diff'].split(','),dtype = float))\n",
    "    sorting.append(np.array(df.loc[q, '_SORTING'].split(','),dtype = float))\n",
    "print('Traccie: ',len(traces))\n",
    "\n",
    "rda = df['RKDST']\n",
    "werks_bef = df['WERKS']\n",
    "name1_bef = df['NAME1']\n",
    "matkl_bef = df['MATKL']\n",
    "matnr_bef = df['MATNR']\n",
    "mavp = df['MAVPR']\n",
    "twms = df['THWMS']\n",
    "lates = df['LATES']\n",
    "lated = df['LATED']\n",
    "latep = df['LATEP']\n",
    "budat = df['time:diff_BUDAT']\n",
    "bpdat = df['time:diff_BPDAT']\n",
    "lewed = df['time:diff_LEWED']\n",
    "sedat = df['time:diff_SEDAT']\n",
    "lcwed = df['time:diff_LCWED']\n",
    "itdat = df['time:diff_ITDAT']\n",
    "\n",
    "werks = onehot_encoder_wer.transform(np.array(werks_bef).reshape(-1, 1))\n",
    "name1 = onehot_encoder_nam.transform(np.array(name1_bef).reshape(-1, 1))\n",
    "matkl = onehot_encoder_mag.transform(np.array(matkl_bef).reshape(-1, 1))\n",
    "matnr = onehot_encoder_mat.transform(np.array(matnr_bef).reshape(-1, 1))\n",
    "\n",
    "## RDA\n",
    "for l in range(len(traces)):\n",
    "    to_app = np.array([[rda[l]] * len(traces[l])]).reshape(len(traces[l]), 1)\n",
    "    traces[l] = np.append(traces[l], to_app, axis = 1)\n",
    "# WERKS\n",
    "for l in range(len(traces)):\n",
    "    to_app = np.array([[werks[l]] * len(traces[l])]).reshape(len(traces[l]), n_wer)\n",
    "    traces[l] = np.append(traces[l], to_app, axis = 1)\n",
    "# NAME1\n",
    "for l in range(len(traces)):\n",
    "    to_app = np.array([[name1[l]] * len(traces[l])]).reshape(len(traces[l]), n_nam)\n",
    "    traces[l] = np.append(traces[l], to_app, axis = 1)\n",
    "# MATKL\n",
    "for l in range(len(traces)):\n",
    "    to_app = np.array([[matkl[l]] * len(traces[l])]).reshape(len(traces[l]), n_mag)\n",
    "    traces[l] = np.append(traces[l], to_app, axis = 1)\n",
    "# MATNR\n",
    "for l in range(len(traces)):\n",
    "    to_app = np.array([[matnr[l]] * len(traces[l])]).reshape(len(traces[l]), n_mat)\n",
    "    traces[l] = np.append(traces[l], to_app, axis = 1)\n",
    "# TIME\n",
    "for l in range(len(traces)):\n",
    "    to_app = diff_time[l].reshape(len(traces[l]),1)\n",
    "    traces[l] = np.append(traces[l], to_app, axis = 1)\n",
    "# MAVP\n",
    "for l in range(len(traces)):\n",
    "    to_app = np.array([[mavp[l]] * len(traces[l])]).reshape(len(traces[l]), 1)\n",
    "    traces[l] = np.append(traces[l], to_app, axis = 1)\n",
    "# TWMS\n",
    "for l in range(len(traces)):\n",
    "    to_app = np.array([[twms[l]] * len(traces[l])]).reshape(len(traces[l]), 1)\n",
    "    traces[l] = np.append(traces[l], to_app, axis = 1)\n",
    "# LATES\n",
    "for l in range(len(traces)):\n",
    "    to_app = np.array([[lates[l]] * len(traces[l])]).reshape(len(traces[l]), 1)\n",
    "    traces[l] = np.append(traces[l], to_app, axis = 1)\n",
    "# LATED\n",
    "for l in range(len(traces)):\n",
    "    to_app = np.array([[lated[l]] * len(traces[l])]).reshape(len(traces[l]), 1)\n",
    "    traces[l] = np.append(traces[l], to_app, axis = 1)\n",
    "# LATEP\n",
    "for l in range(len(traces)):\n",
    "    to_app = np.array([[latep[l]] * len(traces[l])]).reshape(len(traces[l]), 1)\n",
    "    traces[l] = np.append(traces[l], to_app, axis = 1)\n",
    "# BUDAT\n",
    "for l in range(len(traces)):\n",
    "    to_app = np.array([[budat[l]] * len(traces[l])]).reshape(len(traces[l]), 1)\n",
    "    traces[l] = np.append(traces[l], to_app, axis = 1)\n",
    "# BPDAT\n",
    "for l in range(len(traces)):\n",
    "    to_app = np.array([[bpdat[l]] * len(traces[l])]).reshape(len(traces[l]), 1)\n",
    "    traces[l] = np.append(traces[l], to_app, axis = 1)\n",
    "# LEWED\n",
    "for l in range(len(traces)):\n",
    "    to_app = np.array([[lewed[l]] * len(traces[l])]).reshape(len(traces[l]), 1)\n",
    "    traces[l] = np.append(traces[l], to_app, axis = 1)\n",
    "# SEDAT\n",
    "for l in range(len(traces)):\n",
    "    to_app = np.array([[sedat[l]] * len(traces[l])]).reshape(len(traces[l]), 1)\n",
    "    traces[l] = np.append(traces[l], to_app, axis = 1)\n",
    "# LCWED\n",
    "for l in range(len(traces)):\n",
    "    to_app = np.array([[lcwed[l]] * len(traces[l])]).reshape(len(traces[l]), 1)\n",
    "    traces[l] = np.append(traces[l], to_app, axis = 1)\n",
    "# ITDAT\n",
    "for l in range(len(traces)):\n",
    "    to_app = np.array([[itdat[l]] * len(traces[l])]).reshape(len(traces[l]), 1)\n",
    "    traces[l] = np.append(traces[l], to_app, axis = 1)\n",
    "# # SORTING\n",
    "# for l in range(len(traces)):\n",
    "#     to_app = sorting[l].reshape(len(traces[l]),1)\n",
    "#     traces[l] = np.append(traces[l], to_app, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process instances creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "n_min = 1\n",
    "n_zero = traces[0].shape[1]\n",
    "\n",
    "for q in range(len(traces)):\n",
    "    sequence = traces[q]\n",
    "    if sequence.shape[0] > n_min:\n",
    "        for windows in range(sequence.shape[0] - n_min):\n",
    "            X.append(sequence[0:windows + n_min, :])\n",
    "            y.append(sequence[windows + n_min: windows + n_min+1, :])\n",
    "print('Numbero di X: ', len(X))\n",
    "\n",
    "max_trace = find_max_list(traces)\n",
    "print(max_trace)\n",
    "\n",
    "for i in range(len(X)):\n",
    "    num = max_trace - len(X[i])\n",
    "    for n in range(num):\n",
    "        X[i] = np.append(X[i], [[0] * n_zero],  axis=0)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=30)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=1)\n",
    "\n",
    "print('Train shape: ', len(X_train))\n",
    "print('Val shape: ', len(X_test))\n",
    "print('Test shape: ', len(X_val))\n",
    "\n",
    "X_rda_train = X_train[:,0,n_act].reshape(-1,1)\n",
    "X_rda_test = X_test[:,0,n_act].reshape(-1,1)\n",
    "X_rda_val = X_val[:,0,n_act].reshape(-1,1)\n",
    "\n",
    "X_wer_train = X_train[:,0,n_act+1 : n_act+1+n_wer]\n",
    "X_wer_test = X_test[:,0,n_act+1 : n_act+1+n_wer]\n",
    "X_wer_val = X_val[:,0,n_act+1 : n_act+1+n_wer]\n",
    "\n",
    "X_nam_train = X_train[:,0,n_act+1+n_wer : n_act+1+n_wer+n_nam]\n",
    "X_nam_test = X_test[:,0,n_act+1+n_wer : n_act+1+n_wer+n_nam]\n",
    "X_nam_val = X_val[:,0,n_act+1+n_wer : n_act+1+n_wer+n_nam]\n",
    "\n",
    "X_mag_train = X_train[:,0,n_act+1+n_wer+n_nam : n_act+1+n_wer+n_nam+n_mag]\n",
    "X_mag_test = X_test[:,0,n_act+1+n_wer+n_nam : n_act+1+n_wer+n_nam+n_mag]\n",
    "X_mag_val = X_val[:,0,n_act+1+n_wer+n_nam : n_act+1+n_wer+n_nam+n_mag]\n",
    "\n",
    "X_mat_train = X_train[:,0,n_act+1+n_wer+n_nam+n_mag : n_act+1+n_wer+n_nam+n_mag+n_mat]\n",
    "X_mat_test = X_test[:,0,n_act+1+n_wer+n_nam+n_mag : n_act+1+n_wer+n_nam+n_mag+n_mat]\n",
    "X_mat_val = X_val[:,0,n_act+1+n_wer+n_nam+n_mag : n_act+1+n_wer+n_nam+n_mag+n_mat]\n",
    "\n",
    "X_time_train = X_train[:,:, n_act+1+n_wer+n_nam+n_mag+n_mat].reshape(-1,max_trace,1)\n",
    "X_time_test = X_test[:,:, n_act+1+n_wer+n_nam+n_mag+n_mat].reshape(-1,max_trace,1)\n",
    "X_time_val = X_val[:,:, n_act+1+n_wer+n_nam+n_mag+n_mat].reshape(-1,max_trace,1)\n",
    "\n",
    "X_budat_train = X_train[:,0, n_act+1+n_wer+n_nam+n_mag+n_mat+6].reshape(-1,1)\n",
    "X_budat_test = X_test[:,0, n_act+1+n_wer+n_nam+n_mag+n_mat+6].reshape(-1,1)\n",
    "X_budat_val = X_val[:,0, n_act+1+n_wer+n_nam+n_mag+n_mat+6].reshape(-1,1)\n",
    "\n",
    "X_bpdat_train = X_train[:,0, n_act+1+n_wer+n_nam+n_mag+n_mat+7].reshape(-1,1)\n",
    "X_bpdat_test = X_test[:,0, n_act+1+n_wer+n_nam+n_mag+n_mat+7].reshape(-1,1)\n",
    "X_bpdat_val = X_val[:,0, n_act+1+n_wer+n_nam+n_mag+n_mat+7].reshape(-1,1)\n",
    "\n",
    "X_lewed_train = X_train[:,0, n_act+1+n_wer+n_nam+n_mag+n_mat+8].reshape(-1,1)\n",
    "X_lewed_test = X_test[:,0, n_act+1+n_wer+n_nam+n_mag+n_mat+8].reshape(-1,1)\n",
    "X_lewed_val = X_val[:,0, n_act+1+n_wer+n_nam+n_mag+n_mat+8].reshape(-1,1)\n",
    "\n",
    "X_sedat_train = X_train[:,0, n_act+1+n_wer+n_nam+n_mag+n_mat+9].reshape(-1,1)\n",
    "X_sedat_test = X_test[:,0, n_act+1+n_wer+n_nam+n_mag+n_mat+9].reshape(-1,1)\n",
    "X_sedat_val = X_val[:,0, n_act+1+n_wer+n_nam+n_mag+n_mat+9].reshape(-1,1)\n",
    "\n",
    "X_lcwed_train = X_train[:,0, n_act+1+n_wer+n_nam+n_mag+n_mat+10].reshape(-1,1)\n",
    "X_lcwed_test = X_test[:,0, n_act+1+n_wer+n_nam+n_mag+n_mat+10].reshape(-1,1)\n",
    "X_lcwed_val = X_val[:,0, n_act+1+n_wer+n_nam+n_mag+n_mat+10].reshape(-1,1)\n",
    "\n",
    "X_itdat_train = X_train[:,0, n_act+1+n_wer+n_nam+n_mag+n_mat+11].reshape(-1,1)\n",
    "X_itdat_test = X_test[:,0, n_act+1+n_wer+n_nam+n_mag+n_mat+11].reshape(-1,1)\n",
    "X_itdat_val = X_val[:,0, n_act+1+n_wer+n_nam+n_mag+n_mat+11].reshape(-1,1)\n",
    "\n",
    "# Y\n",
    "\n",
    "\n",
    "y_bpdat_train = y_train[:,0, n_act+1+n_wer+n_nam+n_mag+n_mat+7].reshape(-1,1)\n",
    "y_bpdat_test = y_test[:,0, n_act+1+n_wer+n_nam+n_mag+n_mat+7].reshape(-1,1)\n",
    "y_bpdat_val = y_val[:,0, n_act+1+n_wer+n_nam+n_mag+n_mat+7].reshape(-1,1)\n",
    "\n",
    "y_time_train = y_train[:,:, n_act+1+n_wer+n_nam+n_mag+n_mat].reshape(-1,1)\n",
    "y_time_test = y_test[:,:, n_act+1+n_wer+n_nam+n_mag+n_mat].reshape(-1,1)\n",
    "y_time_val = y_val[:,:, n_act+1+n_wer+n_nam+n_mag+n_mat].reshape(-1,1)\n",
    "\n",
    "y_mav_train = y_train[:,0, n_act+1+n_wer+n_nam+n_mag+n_mat+1].reshape(-1,1)\n",
    "y_mav_test = y_test[:,0, n_act+1+n_wer+n_nam+n_mag+n_mat+1].reshape(-1,1)\n",
    "y_mav_val = y_val[:,0, n_act+1+n_wer+n_nam+n_mag+n_mat+1].reshape(-1,1)\n",
    "\n",
    "y_twm_train = y_train[:,0, n_act+1+n_wer+n_nam+n_mag+n_mat+2].reshape(-1,1)\n",
    "y_twm_test = y_test[:,0, n_act+1+n_wer+n_nam+n_mag+n_mat+2].reshape(-1,1)\n",
    "y_twm_val = y_val[:,0, n_act+1+n_wer+n_nam+n_mag+n_mat+2].reshape(-1,1)\n",
    "\n",
    "y_lates_train = y_train[:,0, n_act+1+n_wer+n_nam+n_mag+n_mat+3].reshape(-1,1)\n",
    "y_lates_test = y_test[:,0, n_act+1+n_wer+n_nam+n_mag+n_mat+3].reshape(-1,1)\n",
    "y_lates_val = y_val[:,0, n_act+1+n_wer+n_nam+n_mag+n_mat+3].reshape(-1,1)\n",
    "\n",
    "y_lated_train = y_train[:,0, n_act+1+n_wer+n_nam+n_mag+n_mat+4].reshape(-1,1)\n",
    "y_lated_test = y_test[:,0, n_act+1+n_wer+n_nam+n_mag+n_mat+4].reshape(-1,1)\n",
    "y_lated_val = y_val[:,0, n_act+1+n_wer+n_nam+n_mag+n_mat+4].reshape(-1,1)\n",
    "\n",
    "y_latep_train = y_train[:,0, n_act+1+n_wer+n_nam+n_mag+n_mat+5].reshape(-1,1)\n",
    "y_latep_test = y_test[:,0, n_act+1+n_wer+n_nam+n_mag+n_mat+5].reshape(-1,1)\n",
    "y_latep_val = y_val[:,0, n_act+1+n_wer+n_nam+n_mag+n_mat+5].reshape(-1,1)\n",
    "\n",
    "X_train = X_train[:,:,:n_act]\n",
    "X_test = X_test[:,:,:n_act]\n",
    "X_val = X_val[:,:,:n_act]\n",
    "# Commentare riga per pi√π output\n",
    "y_train = y_train[:,:,:n_act].reshape(-1,n_act)\n",
    "y_test = y_test[:,:,:n_act].reshape(-1,n_act)\n",
    "y_val = y_val[:,:,:n_act].reshape(-1,n_act)\n",
    "\n",
    "class_weights = compute_class_weight(class_weight = \"balanced\", classes= np.unique(y_train.argmax(axis = 1)), y= y_train.reshape(-1,n_act).argmax(axis = 1))\n",
    "#class_weights = np.insert(class_weights, 3, 0)\n",
    "np.save('class_weights.npy', class_weights)\n",
    "loss = weighted_categorical_crossentropy(class_weights)\n",
    "\n",
    "# from sklearn.utils.class_weight import compute_class_weight\n",
    "# class_weights = compute_class_weight(class_weight = \"balanced\", classes= np.unique(X_train.reshape(-1,11)[np.any(X_train.reshape(-1,11) != [0]*11, axis=1)].argmax(axis = 1)), y= X_train.reshape(-1,11)[np.any(X_train.reshape(-1,11) != [0]*11, axis=1)].argmax(axis = 1))\n",
    "# #class_weights = np.insert(class_weights, 3, 0)\n",
    "\n",
    "# convertire to integer encoding\n",
    "X_wer_train_lab = X_wer_train.argmax(axis = 1).reshape(-1,1)\n",
    "X_wer_val_lab = X_wer_val.argmax(axis = 1).reshape(-1,1)\n",
    "X_wer_test_lab = X_wer_test.argmax(axis = 1).reshape(-1,1)\n",
    "\n",
    "X_nam_train_lab = X_nam_train.argmax(axis = 1).reshape(-1,1)\n",
    "X_nam_val_lab = X_nam_val.argmax(axis = 1).reshape(-1,1)\n",
    "X_nam_test_lab = X_nam_test.argmax(axis = 1).reshape(-1,1)\n",
    "\n",
    "X_mag_train_lab = X_mag_train.argmax(axis = 1).reshape(-1,1)\n",
    "X_mag_val_lab = X_mag_val.argmax(axis = 1).reshape(-1,1)\n",
    "X_mag_test_lab = X_mag_test.argmax(axis = 1).reshape(-1,1)\n",
    "\n",
    "X_mat_train_lab = X_mat_train.argmax(axis = 1).reshape(-1,1)\n",
    "X_mat_val_lab = X_mat_val.argmax(axis = 1).reshape(-1,1)\n",
    "X_mat_test_lab = X_mat_test.argmax(axis = 1).reshape(-1,1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = tf.keras.initializers.GlorotUniform(seed = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'model_suffix_act_2'\n",
    "\n",
    "main_input = Input(shape=(max_trace, n_act), name='main_input')\n",
    "m1 = Masking(mask_value=0)(main_input)\n",
    "l1 = SimpleRNN(32, return_sequences=False, dropout = 0, kernel_initializer=initializer)(m1)\n",
    "time_input = Input(shape=(max_trace, 1), name='time_input')\n",
    "m2 = Masking(mask_value=0)(time_input)\n",
    "l2 = SimpleRNN(16, return_sequences=False, dropout = 0, kernel_initializer=initializer)(m2)\n",
    "\n",
    "c1 = Concatenate()([l1, l2])\n",
    "d1 = Dense(16, activation = 'relu', kernel_initializer=initializer)(c1)\n",
    "\n",
    "pr_input = Input(shape=(1), name='pr_input')\n",
    "we_input = Input(shape=(n_wer), name = 'we_input')\n",
    "na_input = Input(shape=(n_nam), name = 'na_input')\n",
    "mg_input = Input(shape=(n_mag), name='mg_input')\n",
    "mt_input = Input(shape=(n_mat), name = 'mt_input')\n",
    "\n",
    "c2 = Concatenate()([pr_input,\n",
    "                    we_input,\n",
    "                    na_input,\n",
    "                    mg_input,\n",
    "                    mt_input,                    \n",
    "                    d1])\n",
    "\n",
    "dp1 = Dropout(0.2)(c2)\n",
    "d_act = Dense(n_act, kernel_initializer=initializer)(dp1)\n",
    "act_output = Softmax(name='act_output')(d_act)\n",
    "\n",
    "d_mav = Dense(1, kernel_initializer=initializer)(c2)\n",
    "mav_output = keras.layers.Activation(activations.sigmoid, name='mav_output')(d_mav)\n",
    "\n",
    "model = Model([main_input,\n",
    "               time_input,\n",
    "               pr_input,\n",
    "               we_input,\n",
    "               na_input,\n",
    "               mg_input,\n",
    "               mt_input\n",
    "               ],\n",
    "               [act_output,\n",
    "                mav_output,\n",
    "                ])\n",
    "opt = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "loss = weighted_categorical_crossentropy(class_weights)\n",
    "model.compile(loss={'act_output': loss, #'time_output': 'mean_absolute_error',\n",
    "                    'mav_output': 'binary_crossentropy',\n",
    "                    },\n",
    "             optimizer=opt)\n",
    "early_stopping = EarlyStopping(monitor='val_act_output_loss', patience=10)\n",
    "mcp_save = ModelCheckpoint(\"model/\" + name + \".h5\", save_best_only=True, monitor='val_loss', mode='min')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from livelossplot import PlotLossesKeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit([X_train,\n",
    "                     X_time_train,\n",
    "                     X_rda_train,\n",
    "                     X_wer_train,\n",
    "                     X_nam_train,\n",
    "                     X_mag_train,\n",
    "                     X_mat_train\n",
    "                     ],\n",
    "                     [y_train,\n",
    "                      y_mav_train],\n",
    "                    epochs=100,\n",
    "                    batch_size=64,\n",
    "                    shuffle=True,\n",
    "                    validation_data=([X_val,\n",
    "                                      X_time_val,\n",
    "                                      X_rda_val,\n",
    "                                      X_wer_val,\n",
    "                                      X_nam_val,\n",
    "                                      X_mag_val,\n",
    "                                      X_mat_val\n",
    "                     ],\n",
    "                     [y_val, y_mav_val\n",
    "                      ]),\n",
    "                    callbacks = [early_stopping, mcp_save, PlotLossesKeras()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('model/model_suffix_act.h5', custom_objects = {'loss' : loss})\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat, yhat_mavp = model.predict([X_test,\n",
    "                                 X_time_test,\n",
    "                                 X_rda_test,\n",
    "                                 X_wer_test,\n",
    "                                 X_nam_test,\n",
    "                                 X_mag_test,\n",
    "                                 X_mat_test\n",
    "                                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "T5_labels = onehot_encoder_act.inverse_transform(np.identity(n_act))[:,0]\n",
    "\n",
    "cm = confusion_matrix(y_test.argmax(axis = 1), yhat.argmax(axis = 1), labels = np.identity(n_act).argmax(axis = 1), normalize = 'true')\n",
    "df_cm = pd.DataFrame(cm, range(n_act), range(n_act))\n",
    "sns.set(font_scale = 1.4)\n",
    "g = sns.heatmap(df_cm, annot = True, annot_kws={'size' : 10},\n",
    "                xticklabels = T5_labels, yticklabels= T5_labels,\n",
    "                #vmin = 0, vmax = 1, fmt=\".0%\",\n",
    "                cmap ='Blues',\n",
    "                square = True, linewidths=1.4, cbar_kws={\"shrink\" : .8}, ax = ax)\n",
    "g.set_xticklabels(g.get_xmajorticklabels(), fontsize = 10)\n",
    "g.set_yticklabels(g.get_xmajorticklabels(), fontsize = 10) \n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def X_2_activity(X, y):\n",
    "    X_2 = X.copy()\n",
    "    for i in range(len(X_2)):\n",
    "        for q in range(max_trace):\n",
    "            if (X_2[i][q] == [0] * n_act).all():\n",
    "                X_2[i][q] = y[i]\n",
    "                break\n",
    "    return X_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'model_suffix_time'\n",
    "\n",
    "main_input = Input(shape=(max_trace, n_act), name='main_input')\n",
    "m1 = Masking(mask_value=0)(main_input)\n",
    "l1 = SimpleRNN(32, return_sequences=False, dropout = 0, kernel_initializer=initializer)(m1)\n",
    "time_input = Input(shape=(max_trace, 1), name='time_input')\n",
    "m2 = Masking(mask_value=0)(time_input)\n",
    "l2 = SimpleRNN(16, return_sequences=False, dropout = 0, kernel_initializer=initializer)(m2)\n",
    "\n",
    "c1 = Concatenate()([l1, l2])\n",
    "d1 = Dense(16, activation = 'relu', kernel_initializer=initializer)(c1)\n",
    "\n",
    "pr_input = Input(shape=(1), name='pr_input')\n",
    "mg_input = Input(shape=(n_mag), name='mg_input')\n",
    "\n",
    "c2 = Concatenate()([pr_input, mg_input, d1])\n",
    "\n",
    "# dp1 = Dropout(0.2)(c2)\n",
    "# d_act = Dense(n_act, kernel_initializer=initializer)(dp1)\n",
    "# act_output = Softmax(name='act_output')(d_act)\n",
    "\n",
    "time_output = Dense(1, name='time_output', kernel_initializer=initializer)(c2)\n",
    "\n",
    "# d_mav = Dense(1, kernel_initializer=initializer)(c2)\n",
    "# mav_output = keras.layers.Activation(activations.sigmoid, name='mav_output')(d_mav)\n",
    "\n",
    "# d_twm = Dense(1, kernel_initializer=initializer)(c2)\n",
    "# twm_output = keras.layers.Activation(activations.sigmoid, name='twm_output')(d_twm)\n",
    "\n",
    "\n",
    "model = Model([main_input, time_input,\n",
    "               pr_input, mg_input,\n",
    "               ],\n",
    "               [time_output\n",
    "                # twm_output, latep_output\n",
    "                ])#, lates_output, lated_output, latep_output])\n",
    "opt = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "loss = weighted_categorical_crossentropy(class_weights)\n",
    "model.compile(loss={\n",
    "    #'act_output': loss, \n",
    "    'time_output': 'mean_absolute_error',\n",
    "    #                'mav_output': 'binary_crossentropy', #'twm_output': 'binary_crossentropy',\n",
    "                    },\n",
    "             optimizer=opt)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "mcp_save = ModelCheckpoint(\"model/\" + name + \".h5\", save_best_only=True, monitor='val_loss', mode='min')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from livelossplot import PlotLossesKeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2_train = X_2_activity(X_train, y_train)\n",
    "X_2_val = X_2_activity(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit([X_2_train, X_time_train,\n",
    "                     X_rda_train, X_mag_train,\n",
    "                     ],\n",
    "                     [y_time_train# , y_twm_train, y_latep_train\n",
    "                      ],# , y_lates_train, y_lated_train, y_bpdat_train],\n",
    "                    epochs=100,\n",
    "                    batch_size=64,\n",
    "                    shuffle=True,\n",
    "                    validation_data=([X_2_val, X_time_val,\n",
    "                     X_rda_val,X_mag_val\n",
    "                     ],\n",
    "                     [y_time_val# , y_twm_val, y_latep_val\n",
    "                      ]),# , y_lates_val, y_lated_val, y_bpdat_val]),\n",
    "                    callbacks = [early_stopping, mcp_save, PlotLossesKeras()])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
